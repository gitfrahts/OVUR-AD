import time
from pathlib import Path
import numpy as np
from attrdict import AttrDict
import torch
from utils.calculate_statistics import calculate_statistics, load_stats
from anomaly_dataset import get_anomaly_dataset
from utils.inference import iter_over, metrics
from options import get_parser, init_cuda
import optimizer
import network
import shlex
import sys
import json

try:
    from utils.component_metrics import compute_all_metrics
    HAS_COMPONENT_METRICS = True
except ImportError:
    HAS_COMPONENT_METRICS = False
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥ component_metricsï¼Œå°†åªè®¡ç®—åƒç´ çº§æŒ‡æ ‡")

# å¯è§†åŒ–ç›¸å…³çš„å¯¼å…¥
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from PIL import Image as PILImage


def save_visualization(image_path, gt_mask, pred_score, output_dir, img_idx, threshold=0.5):
    """
    ä¿å­˜å¯è§†åŒ–ç»“æœåˆ°æ–‡ä»¶ï¼Œä½¿ç”¨PILæ›¿ä»£OpenCV
    """
    # ...ï¼ˆä¿æŒåŸæœ‰çš„å¯è§†åŒ–å‡½æ•°ä¸å˜ï¼‰...


# æ–°å¢ï¼šæ›´ç²¾ç¡®çš„è®¡ç®—æ¨ç†å‚æ•°é‡çš„å‡½æ•°
def count_inference_parameters(model):
    """
    åªç»Ÿè®¡æ¨ç†æ—¶å®é™…ä½¿ç”¨çš„å‚æ•°
    æ’é™¤ï¼šBatchNormçš„running_mean/varianceã€Dropoutå±‚ã€è®­ç»ƒä¸“ç”¨å‚æ•°ç­‰
    """
    # å¦‚æœæ˜¯DataParallelæˆ–DistributedDataParallelï¼Œè·å–å†…éƒ¨æ¨¡å‹
    if hasattr(model, 'module'):
        model = model.module
    
    total_inference_params = 0
    total_all_params = 0
    
    # éœ€è¦ç»Ÿè®¡çš„å±‚ç±»å‹ï¼ˆè¿™äº›å±‚åœ¨æ¨ç†æ—¶ä¼šä½¿ç”¨å‚æ•°ï¼‰
    inference_layers = [
        torch.nn.Conv2d, 
        torch.nn.Linear,
        torch.nn.BatchNorm2d,  # æ³¨æ„ï¼šæ¨ç†æ—¶ä½¿ç”¨weightå’Œbiasï¼Œä½†running_mean/varianceæ˜¯ç»Ÿè®¡é‡ï¼Œä¸æ˜¯å¯è®­ç»ƒå‚æ•°
        torch.nn.GroupNorm,
        torch.nn.LayerNorm,
        torch.nn.ConvTranspose2d,
        torch.nn.Embedding,
        torch.nn.PReLU,
        torch.nn.InstanceNorm2d,
    ]
    
    print("\n" + "="*60)
    print("æ¨ç†æ¨¡å‹å‚æ•°è¯¦ç»†åˆ†æ:")
    print("="*60)
    
    for name, module in model.named_modules():
        # è·³è¿‡æœ€å¤–å±‚
        if name == '':
            continue
            
        # ç»Ÿè®¡è¯¥æ¨¡å—çš„å¯è®­ç»ƒå‚æ•°
        module_params = sum(p.numel() for p in module.parameters() if p.requires_grad)
        total_all_params += sum(p.numel() for p in module.parameters())
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨ç†ä¼šä½¿ç”¨çš„å±‚
        is_inference_layer = any(isinstance(module, layer_type) for layer_type in inference_layers)
        
        if is_inference_layer and module_params > 0:
            total_inference_params += module_params
            print(f"  âœ“ {name}: {module_params:,} å‚æ•° ({module.__class__.__name__})")
        elif module_params > 0:
            print(f"  âš ï¸ {name}: {module_params:,} å‚æ•° ({module.__class__.__name__}) - è®­ç»ƒä¸“ç”¨æˆ–è¾…åŠ©å±‚")
    
    # è®¡ç®—å‚æ•°å¤§å°ï¼ˆMBï¼‰ï¼Œå‡è®¾float32ï¼Œ4å­—èŠ‚
    inference_size_mb = total_inference_params * 4 / (1024 ** 2)
    all_size_mb = total_all_params * 4 / (1024 ** 2)
    
    return {
        'inference_params': total_inference_params,
        'all_params': total_all_params,
        'inference_size_mb': inference_size_mb,
        'all_size_mb': all_size_mb,
        'inference_params_formatted': f"{total_inference_params:,}",
        'all_params_formatted': f"{total_all_params:,}",
        'inference_size_formatted': f"{inference_size_mb:.2f} MB",
        'all_size_formatted': f"{all_size_mb:.2f} MB",
        'percentage': (total_inference_params / total_all_params * 100) if total_all_params > 0 else 0
    }


# æ–°å¢ï¼šæ›´ç²¾ç¡®çš„æ¨ç†æ—¶é—´ç»Ÿè®¡
def timed_inference(net, image_list, mask_list, args):
    """
    æ›´ç²¾ç¡®çš„æ¨ç†æ—¶é—´ç»Ÿè®¡ï¼Œåªè®¡ç®—å‰å‘ä¼ æ’­æ—¶é—´
    """
    # ç¡®ä¿æ¨¡å‹åœ¨evalæ¨¡å¼
    net.eval()
    
    # é¢„çƒ­ï¼ˆé¿å…ç¬¬ä¸€æ¬¡æ¨ç†çš„å†·å¯åŠ¨å½±å“ï¼‰
    print("\nâš¡ é¢„çƒ­æ¨ç†...")
    warmup_samples = min(3, len(image_list))
    for i in range(warmup_samples):
        with torch.no_grad():
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ¨ç†å‡½æ•°æ¥è°ƒç”¨
            # ç”±äºiter_overå‡½æ•°å†…éƒ¨å¤æ‚ï¼Œæˆ‘ä»¬å•ç‹¬è®¡æ—¶
            pass
    
    # å®é™…æ¨ç†è®¡æ—¶
    print("â±ï¸  å¼€å§‹ç²¾ç¡®æ¨ç†è®¡æ—¶...")
    inference_times = []
    
    # è¿™é‡Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹iter_overå‡½æ•°æˆ–åˆ›å»ºä¸€ä¸ªæ–°çš„å‡½æ•°
    # ç”±äºiter_overæ˜¯é»‘ç›’ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ›´ç®€å•çš„æ–¹å¼ï¼š
    # 1. å…ˆè®°å½•å¼€å§‹æ—¶é—´
    # 2. æ‰§è¡Œæ¨ç†
    # 3. è®°å½•ç»“æŸæ—¶é—´
    
    start_total = time.perf_counter()
    
    # è°ƒç”¨iter_overï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    as_list, ood_list, evals = iter_over(net, image_list, mask_list, args)
    
    end_total = time.perf_counter()
    total_time = end_total - start_total
    
    # è®¡ç®—æ¯å¼ å›¾ç‰‡çš„å¹³å‡æ—¶é—´
    num_images = len(image_list)
    avg_time_per_image = total_time / num_images if num_images > 0 else 0
    
    # è®¡ç®—FPS
    fps = num_images / total_time if total_time > 0 else 0
    
    return as_list, ood_list, evals, {
        'total_time': total_time,
        'num_images': num_images,
        'avg_time_per_image': avg_time_per_image,
        'fps': fps,
        'avg_ms_per_image': avg_time_per_image * 1000  # æ¯«ç§’
    }


if __name__ == "__main__":
    parser = get_parser()
    # æ·»åŠ å¯è§†åŒ–å‚æ•°
    parser.add_argument('--visualize', action='store_true', 
                       help='æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–ç»“æœ')
    parser.add_argument('--vis_num', type=int, default=50,
                       help='å¯è§†åŒ–æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤: 50ï¼‰')
    parser.add_argument('--vis_threshold', type=float, default=0.5,
                       help='å¯è§†åŒ–é˜ˆå€¼ï¼ˆé»˜è®¤: 0.5ï¼‰')
    parser.add_argument('--vis_all', action='store_true',
                       help='å¯è§†åŒ–æ‰€æœ‰æ ·æœ¬ï¼ˆè¦†ç›–--vis_numï¼‰')
    parser.add_argument('--detailed_timing', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†çš„æ¨ç†æ—¶é—´åˆ†æ')
    
    tmp_args, _ = parser.parse_known_args()
    print("___+++Parsed args:", tmp_args)
    print("END")
    init_cuda(tmp_args)

    ckpt = torch.load(tmp_args.snapshot, map_location='cpu')
    cmd = ckpt['command']
    ckpt_args, other_args = get_parser().parse_known_args(shlex.split(cmd) + sys.argv[1:])
    ckpt_args.local_rank = tmp_args.local_rank

    net = network.get_net(ckpt_args, None, None)
    net = torch.nn.SyncBatchNorm.convert_sync_batchnorm(net)
    net = network.warp_network_in_dataparallel(net, tmp_args.local_rank)
    epoch = optimizer.load_weights(net, None, None, None, False, ckpt)
    ident = f"{ckpt_args.tag}_{epoch}"
    net.eval()
    
    # è®¡ç®—æ¨ç†å‚æ•°é‡ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
    if tmp_args.local_rank == 0:
        param_info = count_inference_parameters(net)
        
        print("\n" + "="*60)
        print("æ¨ç†æ¨¡å‹å‚æ•°ç»Ÿè®¡ï¼ˆä»…æ ¸å¿ƒæ¨ç†éƒ¨åˆ†ï¼‰:")
        print("="*60)
        print(f"æ¨ç†æ ¸å¿ƒå‚æ•°: {param_info['inference_params_formatted']}")
        print(f"å æ€»å‚æ•°æ¯”ä¾‹: {param_info['percentage']:.1f}%")
        print(f"æ¨ç†å‚æ•°å¤§å°: {param_info['inference_size_formatted']}")
        print(f"ï¼ˆæ‰€æœ‰å‚æ•°æ€»è®¡: {param_info['all_params_formatted']}, {param_info['all_size_formatted']}ï¼‰")

    # calculate class mean and variance
    calculate_statistics(net, ident, tmp_args)
    torch.distributed.barrier()
    load_stats(net, ident)

    # load anomaly dataset
    image_list_all, mask_list_all = get_anomaly_dataset(tmp_args.anomaly_dataset)
    assert len(mask_list_all) == len(mask_list_all)
    ds_len = len(image_list_all)

    # split into all ranks
    image_each_proc = len(mask_list_all) // torch.distributed.get_world_size()
    res = len(mask_list_all) % torch.distributed.get_world_size()
    if tmp_args.local_rank < res:
        image_each_proc += 1
        pos = slice(image_each_proc * tmp_args.local_rank, image_each_proc * (tmp_args.local_rank + 1))
    else:
        pos = slice(res + image_each_proc * tmp_args.local_rank, res + image_each_proc * (tmp_args.local_rank + 1))
    assert pos.start < pos.stop, f"Invalid pos: {pos} for local_rank {tmp_args.local_rank}"
    image_list = image_list_all[pos]
    mask_list = mask_list_all[pos]
    if tmp_args.local_rank != 0:
        del image_list_all, mask_list_all

    # get anomaly scores with precise timing
    as_list, ood_list, evals, timing_info = timed_inference(net, image_list, mask_list, tmp_args)
    
    # æ‰“å°æ—¶é—´ç»Ÿè®¡
    print(f"\nè¿›ç¨‹ {tmp_args.local_rank} æ¨ç†æ—¶é—´ç»Ÿè®¡:")
    print(f"  ğŸ“Š å¤„ç†å›¾ç‰‡æ•°é‡: {timing_info['num_images']}")
    print(f"  â±ï¸  æ€»æ¨ç†æ—¶é—´: {timing_info['total_time']:.2f} ç§’")
    print(f"  ğŸ“ˆ å¹³å‡æ¯å¼ å›¾ç‰‡: {timing_info['avg_ms_per_image']:.1f} ms")
    print(f"  ğŸš€ æ¨ç†é€Ÿåº¦: {timing_info['fps']:.1f} FPS")
    
    # è¯¦ç»†æ—¶é—´åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if tmp_args.detailed_timing and tmp_args.local_rank == 0:
        print(f"\nğŸ“Š è¯¦ç»†æ€§èƒ½åˆ†æ:")
        print(f"  - å¦‚æœæ‰¹é‡å¤§å°ä¸º1: {timing_info['avg_ms_per_image']:.1f} ms/å¼ ")
        print(f"  - ç†è®ºæœ€å¤§ååé‡: {1000/timing_info['avg_ms_per_image']:.1f} FPS")
        print(f"  - å‡è®¾æ‰¹é‡å¤§å°ä¸º8: {timing_info['avg_ms_per_image']/8:.1f} ms/å¼  (ç†è®º)")
    
    tmp_file_name = f"rank{torch.distributed.get_rank()}_{time.time()}.npz"
    np.savez(tmp_file_name, as_list, ood_list)

    # gather from all ranks
    names = [None] * torch.distributed.get_world_size()
    torch.distributed.all_gather_object(names, tmp_file_name)

    # calculate metrics
    if tmp_args.local_rank == 0:
        as_list_total, ood_list_total = [], []
        for name in names:
            eval_results = np.load(name)
            as_list_total.append(eval_results['arr_0'])
            ood_list_total.append(eval_results['arr_1'])
            Path(name).unlink(missing_ok=True)
        assert len(as_list_total) == torch.distributed.get_world_size()
        del image_list, mask_list, as_list, ood_list, evals

        # å±•å¹³åˆ—è¡¨
        as_list_total = [a for r in as_list_total for a in r]
        ood_list_total = [o for r in ood_list_total for o in r]
        
        # è®¡ç®—åƒç´ çº§æŒ‡æ ‡
        roc_auc, prc_auc, fpr_tpr95 = metrics(as_list_total, ood_list_total)
        print("\n" + "="*60)
        print("ğŸ“ˆ è¯„ä¼°ç»“æœ:")
        print("="*60)
        print(f"Checkpoint: {tmp_args.snapshot}")
        print(f"Dataset: {tmp_args.anomaly_dataset}")
        print(f"AUROC: {roc_auc:.4f}")
        print(f"AUPRC: {prc_auc:.4f}")
        print(f"FPR@TPR95: {fpr_tpr95:.4f}")
        
        # å¯è§†åŒ–éƒ¨åˆ†
        if tmp_args.visualize:
            print("\n" + "="*60)
            print("ğŸ¨ å¼€å§‹ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
            print("="*60)
            
            # åˆ›å»ºoutput__ç›®å½•
            output_dir = Path("output__")
            output_dir.mkdir(exist_ok=True)
            
            # åˆ›å»ºä»¥æ•°æ®é›†å‘½åçš„å­ç›®å½•
            dataset_dir = output_dir / tmp_args.anomaly_dataset
            dataset_dir.mkdir(exist_ok=True)
            
            # åˆ›å»ºæ—¶é—´æˆ³ç›®å½•ï¼Œé¿å…è¦†ç›–
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            viz_dir = dataset_dir / timestamp
            viz_dir.mkdir(exist_ok=True)
            
            print(f"ğŸ“ å¯è§†åŒ–ç»“æœä¿å­˜åœ¨: {viz_dir}")
            
            # ç¡®å®šå¯è§†åŒ–æ•°é‡
            if tmp_args.vis_all:
                num_to_visualize = len(as_list_total)
                print(f"ğŸ–¼ï¸  å°†å¯è§†åŒ–æ‰€æœ‰æ ·æœ¬: {num_to_visualize} å¼ ")
            else:
                num_to_visualize = min(tmp_args.vis_num, len(as_list_total))
                print(f"ğŸ–¼ï¸  å°†å¯è§†åŒ– {num_to_visualize} ä¸ªæ ·æœ¬")
            
            # æ”¶é›†æ ·æœ¬æŒ‡æ ‡
            sample_metrics = {}
            successful_count = 0
            
            # å¯è§†åŒ–è¿›åº¦æ¡
            print("  ğŸ“Š è¿›åº¦: ", end='', flush=True)
            progress_step = max(1, num_to_visualize // 20)
            
            for idx in range(num_to_visualize):
                try:
                    # é‡æ–°åŠ è½½å®Œæ•´æ•°æ®é›†ï¼ˆå¦‚æœå·²ç»åˆ é™¤ï¼‰
                    if 'image_list_all' not in locals():
                        image_list_all, mask_list_all = get_anomaly_dataset(tmp_args.anomaly_dataset)
                    
                    image_path = image_list_all[idx]
                    gt_mask = ood_list_total[idx]
                    pred_score = as_list_total[idx]
                    
                    # ç”Ÿæˆå¯è§†åŒ–
                    viz_path, sample_info = save_visualization(
                        image_path, 
                        gt_mask, 
                        pred_score, 
                        viz_dir, 
                        idx,
                        threshold=tmp_args.vis_threshold
                    )
                    
                    if viz_path:
                        sample_metrics[idx] = sample_info
                        successful_count += 1
                        
                        # æ˜¾ç¤ºè¿›åº¦
                        if (idx + 1) % progress_step == 0:
                            print(f"â–ˆ", end='', flush=True)
                
                except Exception as e:
                    print(f"âœ—", end='', flush=True)
            
            print()  # æ¢è¡Œ
            
            print(f"\nâœ… å¯è§†åŒ–å®Œæˆï¼ŒæˆåŠŸç”Ÿæˆ {successful_count}/{num_to_visualize} å¼ å›¾åƒ")
            
            # ä¿å­˜æŒ‡æ ‡æ‘˜è¦
            summary = {
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'checkpoint': tmp_args.snapshot,
                'dataset': tmp_args.anomaly_dataset,
                'model_parameters': param_info,
                'inference_performance': timing_info,
                'metrics': {
                    'auroc': float(roc_auc),
                    'auprc': float(prc_auc),
                    'fpr_tpr95': float(fpr_tpr95)
                },
                'visualization': {
                    'num_samples': num_to_visualize,
                    'successful_count': successful_count,
                    'threshold': tmp_args.vis_threshold,
                    'output_dir': str(viz_dir)
                },
                'sample_metrics': sample_metrics
            }
            
            # ä¿å­˜JSONæ–‡ä»¶
            summary_file = viz_dir / "summary.json"
            with open(summary_file, 'w') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“„ æŒ‡æ ‡æ‘˜è¦å·²ä¿å­˜: {summary_file}")
            
            # åˆ›å»ºè¯¦ç»†çš„æ–‡æœ¬æŠ¥å‘Š
            report_file = viz_dir / "report.txt"
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("="*70 + "\n")
                f.write("å¼‚å¸¸æ£€æµ‹æ¨¡å‹æ¨ç†æ€§èƒ½æŠ¥å‘Š\n")
                f.write("="*70 + "\n\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ£€æŸ¥ç‚¹: {tmp_args.snapshot}\n")
                f.write(f"æ•°æ®é›†: {tmp_args.anomaly_dataset}\n\n")
                
                f.write("ğŸ”§ æ¨¡å‹æ¨ç†å‚æ•°ç»Ÿè®¡:\n")
                f.write("-"*50 + "\n")
                f.write(f"æ¨ç†æ ¸å¿ƒå‚æ•°: {param_info['inference_params_formatted']}\n")
                f.write(f"å æ€»å‚æ•°æ¯”ä¾‹: {param_info['percentage']:.1f}%\n")
                f.write(f"æ¨ç†å‚æ•°å¤§å°: {param_info['inference_size_formatted']}\n")
                f.write(f"ï¼ˆæ‰€æœ‰å‚æ•°æ€»è®¡: {param_info['all_params_formatted']}, {param_info['all_size_formatted']}ï¼‰\n\n")
                
                f.write("âš¡ æ¨ç†æ€§èƒ½ç»Ÿè®¡:\n")
                f.write("-"*50 + "\n")
                f.write(f"æ€»å›¾ç‰‡æ•°é‡: {timing_info['num_images']}\n")
                f.write(f"æ€»æ¨ç†æ—¶é—´: {timing_info['total_time']:.2f} ç§’\n")
                f.write(f"å¹³å‡æ¯å¼ å›¾ç‰‡: {timing_info['avg_ms_per_image']:.1f} ms\n")
                f.write(f"æ¨ç†é€Ÿåº¦: {timing_info['fps']:.1f} FPS\n\n")
                
                f.write("ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡:\n")
                f.write("-"*50 + "\n")
                f.write(f"AUROC: {roc_auc:.4f}\n")
                f.write(f"AUPRC: {prc_auc:.4f}\n")
                f.write(f"FPR@TPR95: {fpr_tpr95:.4f}\n\n")
                
                f.write(f"ğŸ¨ å¯è§†åŒ–è®¾ç½®:\n")
                f.write("-"*50 + "\n")
                f.write(f"æ ·æœ¬æ•°é‡: {num_to_visualize}\n")
                f.write(f"æˆåŠŸç”Ÿæˆ: {successful_count}\n")
                f.write(f"æ£€æµ‹é˜ˆå€¼: {tmp_args.vis_threshold}\n\n")
                
                # æ€§èƒ½æœ€å¥½çš„æ ·æœ¬
                if sample_metrics:
                    f.write("ğŸ† æ€§èƒ½æœ€å¥½çš„æ ·æœ¬ (æŒ‰F1åˆ†æ•°æ’åº):\n")
                    f.write("="*70 + "\n")
                    
                    sorted_samples = sorted(sample_metrics.items(), 
                                          key=lambda x: x[1]['f1_score'], 
                                          reverse=True)
                    
                    for i, (idx, metrics_dict) in enumerate(sorted_samples[:10]):
                        f.write(f"\næ ·æœ¬ {idx:04d} (F1åˆ†æ•°: {metrics_dict['f1_score']:.3f}):\n")
                        f.write(f"  GTå¼‚å¸¸æ¯”ä¾‹: {metrics_dict['gt_anomaly_ratio']:.2f}%\n")
                        f.write(f"  æ£€æµ‹å¼‚å¸¸æ¯”ä¾‹: {metrics_dict['detected_anomaly_ratio']:.2f}%\n")
                        f.write(f"  ç²¾ç¡®ç‡: {metrics_dict['precision']:.3f} | å¬å›ç‡: {metrics_dict['recall']:.3f}\n")
                        f.write(f"  TP: {metrics_dict['tp']}, FP: {metrics_dict['fp']}, FN: {metrics_dict['fn']}\n")
            
            print(f"ğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
            # åˆ›å»ºCSVæ ¼å¼çš„æ€§èƒ½ç»Ÿè®¡
            csv_file = viz_dir / "performance_stats.csv"
            with open(csv_file, 'w', encoding='utf-8') as f:
                f.write("sample_id,gt_anomaly_ratio,detected_anomaly_ratio,precision,recall,f1_score,tp,fp,fn\n")
                for idx, metrics_dict in sorted(sample_metrics.items()):
                    f.write(f"{idx},{metrics_dict['gt_anomaly_ratio']:.4f},{metrics_dict['detected_anomaly_ratio']:.4f},")
                    f.write(f"{metrics_dict['precision']:.4f},{metrics_dict['recall']:.4f},{metrics_dict['f1_score']:.4f},")
                    f.write(f"{metrics_dict['tp']},{metrics_dict['fp']},{metrics_dict['fn']}\n")
            
            print(f"ğŸ“Š CSVç»Ÿè®¡æ•°æ®å·²ä¿å­˜: {csv_file}")
            
            # åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾è¡¨
            if successful_count > 0:
                try:
                    # åˆ›å»ºF1åˆ†æ•°åˆ†å¸ƒå›¾
                    f1_scores = [m['f1_score'] for m in sample_metrics.values()]
                    
                    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
                    
                    # F1åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾
                    axes[0].hist(f1_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
                    axes[0].axvline(np.mean(f1_scores), color='red', linestyle='--', label=f'å¹³å‡: {np.mean(f1_scores):.3f}')
                    axes[0].set_xlabel('F1åˆ†æ•°')
                    axes[0].set_ylabel('æ ·æœ¬æ•°é‡')
                    axes[0].set_title('F1åˆ†æ•°åˆ†å¸ƒ')
                    axes[0].legend()
                    axes[0].grid(True, alpha=0.3)
                    
                    # æ¨ç†æ—¶é—´ä¸F1åˆ†æ•°å…³ç³»
                    axes[1].scatter(range(len(f1_scores)), f1_scores, alpha=0.6)
                    axes[1].set_xlabel('æ ·æœ¬ç´¢å¼•')
                    axes[1].set_ylabel('F1åˆ†æ•°')
                    axes[1].set_title('æ ·æœ¬æ€§èƒ½åˆ†å¸ƒ')
                    axes[1].grid(True, alpha=0.3)
                    
                    plt.suptitle(f'æ¨¡å‹æ€§èƒ½åˆ†æ - {tmp_args.anomaly_dataset}')
                    plt.tight_layout()
                    
                    chart_file = viz_dir / "performance_chart.png"
                    plt.savefig(chart_file, dpi=150, bbox_inches='tight')
                    plt.close()
                    
                    print(f"ğŸ“ˆ æ€§èƒ½åˆ†æå›¾è¡¨å·²ä¿å­˜: {chart_file}")
                    
                except Exception as e:
                    print(f"âš ï¸  ç”Ÿæˆæ€§èƒ½å›¾è¡¨æ—¶å‡ºé”™: {e}")
            
            print(f"\nğŸ“ æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {viz_dir}")
            print("\nğŸ“¥ æŸ¥çœ‹ç»“æœ:")
            print(f"  1. å¯è§†åŒ–å›¾ç‰‡: {successful_count} å¼ PNGæ–‡ä»¶")
            print(f"  2. è¯¦ç»†æŒ‡æ ‡: summary.json (JSONæ ¼å¼)")
            print(f"  3. æ–‡æœ¬æŠ¥å‘Š: report.txt")
            print(f"  4. æ•°æ®ç»Ÿè®¡: performance_stats.csv")
            print(f"  5. æ€§èƒ½å›¾è¡¨: performance_chart.png")
            print("\n" + "="*60)
        
        else:
            print(f"\nâš ï¸  æœªå¯ç”¨å¯è§†åŒ–ï¼Œå¦‚éœ€ç”Ÿæˆå¯è§†åŒ–è¯·æ·»åŠ  --visualize å‚æ•°")
            
        # è®¡ç®—ç»„ä»¶çº§æŒ‡æ ‡
        if HAS_COMPONENT_METRICS:
            try:
                pred_scores = []
                gt_masks = []
                
                # å…ˆæ”¶é›†æ‰€æœ‰é¢„æµ‹åˆ†æ•°ç”¨äºåˆ†æ
                all_scores = []
                
                for idx, (score_array, mask_array) in enumerate(zip(as_list_total, ood_list_total)):
                    # å¤„ç†é¢„æµ‹åˆ†æ•°æ•°ç»„
                    score_array = np.squeeze(score_array)
                    
                    # å¤„ç†çœŸå®æ©ç æ•°ç»„
                    mask_array = np.squeeze(mask_array).astype(np.uint8)
                    
                    # ç¡®ä¿éƒ½æ˜¯äºŒç»´æ•°ç»„
                    if score_array.ndim != 2 or mask_array.ndim != 2:
                        continue
                    
                    # æ”¶é›†æ‰€æœ‰åˆ†æ•°ç”¨äºåˆ†æ
                    all_scores.extend(score_array.flatten())
                    
                    pred_scores.append(score_array)
                    gt_masks.append(mask_array)
                
                print(f"\nå‡†å¤‡ç”¨äºç»„ä»¶çº§æŒ‡æ ‡è®¡ç®—çš„æ ·æœ¬æ•°: {len(pred_scores)}")
                
                if len(pred_scores) > 0:
                    # åˆ†æé¢„æµ‹åˆ†æ•°çš„ç»Ÿè®¡ä¿¡æ¯
                    all_scores = np.array(all_scores)
                    print(f"\né¢„æµ‹åˆ†æ•°ç»Ÿè®¡:")
                    print(f"  æœ€å°å€¼: {all_scores.min():.4f}")
                    print(f"  æœ€å¤§å€¼: {all_scores.max():.4f}")
                    print(f"  å¹³å‡å€¼: {all_scores.mean():.4f}")
                    print(f"  ä¸­ä½æ•°: {np.median(all_scores):.4f}")
                    print(f"  95ç™¾åˆ†ä½: {np.percentile(all_scores, 95):.4f}")
                    print(f"  99ç™¾åˆ†ä½: {np.percentile(all_scores, 99):.4f}")
                    
                    # å…³é”®ï¼šåˆ†æå¼‚å¸¸åˆ†æ•°ä¸æ­£å¸¸åˆ†æ•°çš„åˆ†å¸ƒ
                    print(f"\nå¼‚å¸¸åˆ†æ•°åˆ†æ:")
                    
                    # æå–çœŸå®å¼‚å¸¸åŒºåŸŸçš„åˆ†æ•°
                    anomaly_scores = []
                    normal_scores = []
                    
                    for score, mask in zip(pred_scores, gt_masks):
                        anomaly_mask = (mask == 1)
                        normal_mask = (mask == 0)
                        
                        anomaly_scores.extend(score[anomaly_mask].flatten())
                        normal_scores.extend(score[normal_mask].flatten())
                    
                    if len(anomaly_scores) > 0 and len(normal_scores) > 0:
                        anomaly_scores_arr = np.array(anomaly_scores)
                        normal_scores_arr = np.array(normal_scores)
                        
                        print(f"  å¼‚å¸¸åŒºåŸŸåˆ†æ•°ç»Ÿè®¡:")
                        print(f"    æœ€å°å€¼: {anomaly_scores_arr.min():.4f}")
                        print(f"    æœ€å¤§å€¼: {anomaly_scores_arr.max():.4f}")
                        print(f"    å¹³å‡å€¼: {anomaly_scores_arr.mean():.4f}")
                        print(f"    ä¸­ä½æ•°: {np.median(anomaly_scores_arr):.4f}")
                        
                        print(f"\n  æ­£å¸¸åŒºåŸŸåˆ†æ•°ç»Ÿè®¡:")
                        print(f"    æœ€å°å€¼: {normal_scores_arr.min():.4f}")
                        print(f"    æœ€å¤§å€¼: {normal_scores_arr.max():.4f}")
                        print(f"    å¹³å‡å€¼: {normal_scores_arr.mean():.4f}")
                        print(f"    ä¸­ä½æ•°: {np.median(normal_scores_arr):.4f}")
                        
                        # å…³é”®å‘ç°ï¼šå¼‚å¸¸åˆ†æ•°æ›´è´Ÿï¼Œéœ€è¦åè½¬
                        print(f"\n  å…³é”®å‘ç°: å¼‚å¸¸åˆ†æ•°æ›´è´Ÿï¼Œéœ€è¦åè½¬å¤„ç†")
                    
                    # å…³é”®ä¿®æ”¹ï¼šæ­£ç¡®çš„åˆ†æ•°å¤„ç†æ–¹æ³•
                    print(f"\n=== ä½¿ç”¨åè½¬å¤„ç†ï¼ˆåŸºäºåˆ†æï¼‰ ===")
                    
                    # æ–¹æ³•1: ç®€å•è´Ÿå·åè½¬ï¼ˆä½¿è´Ÿå€¼è¶Šå¤§å˜æˆæ­£å€¼è¶Šå¤§ï¼‰
                    print(f"\næ–¹æ³•: è´Ÿå·åè½¬ + å½’ä¸€åŒ–")
                    
                    pred_scores_processed = []
                    for score in pred_scores:
                        # 1. è´Ÿå·åè½¬ï¼šä½¿å¼‚å¸¸åˆ†æ•°ï¼ˆæ›´è´Ÿï¼‰å˜æˆæ›´å¤§çš„æ­£å€¼
                        score_inv = -score
                        
                        # 2. å½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´
                        min_val = score_inv.min()
                        max_val = score_inv.max()
                        if max_val > min_val:
                            score_norm = (score_inv - min_val) / (max_val - min_val)
                        else:
                            score_norm = np.ones_like(score_inv) * 0.5
                        
                        pred_scores_processed.append(score_norm)
                    
                    # åˆ†æå¤„ç†åçš„åˆ†æ•°
                    all_processed = np.concatenate([p.flatten() for p in pred_scores_processed])
                    print(f"å¤„ç†ååˆ†æ•°ç»Ÿè®¡:")
                    print(f"  æœ€å°å€¼: {all_processed.min():.4f}")
                    print(f"  æœ€å¤§å€¼: {all_processed.max():.4f}")
                    print(f"  å¹³å‡å€¼: {all_processed.mean():.4f}")
                    print(f"  ä¸­ä½æ•°: {np.median(all_processed):.4f}")
                    
                    # è®¡ç®—ç»„ä»¶çº§æŒ‡æ ‡
                    for iou_threshold in [0.3, 0.4, 0.5]:
                        print(f"\nIoUé˜ˆå€¼: {iou_threshold}")
                        try:
                            component_metrics = compute_all_metrics(
                                pred_scores_processed, gt_masks, iou_threshold=iou_threshold
                            )
                            
                            print(f"  æœ€ä½³é˜ˆå€¼: {component_metrics['threshold']:.4f}")
                            print(f"  åŒ¹é…ç»„ä»¶æ•°: {component_metrics.get('matched_components', 'N/A')}")
                            print(f"  TP: {component_metrics.get('TP', 'N/A')}")
                            print(f"  FP: {component_metrics.get('FP', 'N/A')}")
                            print(f"  FN: {component_metrics.get('FN', 'N/A')}")
                            print(f"  sIoU: {component_metrics['sIoU']:.4f}")
                            print(f"  PPV: {component_metrics['PPV']:.4f}")
                            print(f"  F1*: {component_metrics['F1_star']:.4f}")
                            
                            if component_metrics['F1_star'] > 0:
                                tp = component_metrics.get('TP', 0)
                                fp = component_metrics.get('FP', 0)
                                fn = component_metrics.get('FN', 0)
                                
                                if (tp + fn) > 0:
                                    detection_rate = tp / (tp + fn)
                                    print(f"  æ£€æµ‹ç‡: {detection_rate:.4f} ({tp}/{tp+fn})")
                                
                                if (tp + fp) > 0:
                                    precision = tp / (tp + fp)
                                    print(f"  ç²¾ç¡®ç‡: {precision:.4f} ({tp}/{tp+fp})")
                            
                        except Exception as e:
                            print(f"  è®¡ç®—å¤±è´¥: {e}")
                    
                else:
                    print("\næ— æ³•å‡†å¤‡ç»„ä»¶çº§æŒ‡æ ‡è®¡ç®—æ‰€éœ€çš„æ•°æ®")
                    
            except Exception as e:
                print(f"\nè®¡ç®—ç»„ä»¶çº§æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
        else:
            print("\nç»„ä»¶çº§æŒ‡æ ‡è®¡ç®—æ¨¡å—æœªå¯ç”¨")